{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.functional import norm\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, fields, is_test=False, **kwargs):\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.target if not is_test else None\n",
    "            text = row.text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        data_field = fields\n",
    "\n",
    "        #cls == DataFrameDataset\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), data_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), data_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r\"\\#\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"URL\", text)  # remove URL addresses\n",
    "    text = re.sub(r\"@\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/spam.csv\", encoding='latin-1')\n",
    "df = df[['v1', 'v2']]\n",
    "df = df.rename(columns={'v1': 'target', 'v2': 'text'})\n",
    "labels = df['target']\n",
    "texts = df['text']\n",
    "texts = texts.apply(normalize_text)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go until jurong point crazy available only in ...\n",
       "1                                ok lar joking wif u oni \n",
       "2       free entry in 2 a wkly comp to win fa cup fina...\n",
       "3            u dun say so early hor u c already then say \n",
       "4       nah i don't think he goes to usf he lives arou...\n",
       "                              ...                        \n",
       "5567    this is the 2nd time we have tried 2 contact u...\n",
       "5568                   will b going to esplanade fr home?\n",
       "5569    pity was in mood for that so any other suggest...\n",
       "5570    the guy did some bitching but i acted like i'd...\n",
       "5571                            rofl its true to its name\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True,\n",
    "             use_vocab=False,\n",
    "             tokenize=tokenizer\n",
    "             )\n",
    "\n",
    "LABEL = Field(sequential=False,\n",
    "              use_vocab=False,\n",
    "              preprocessing=lambda x: 1 if x == \"spam\" else 0,\n",
    "              is_target=True)\n",
    "\n",
    "fields = [('text', TEXT), ('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DataFrameDataset.splits(fields=fields, train_df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'input_ids': [101, 2175, 2127, 18414, 17583, 2391, 1010, 4689, 1012, 1012, 2800, 2069, 1999, 11829, 2483, 1050, 2307, 2088, 2474, 1041, 28305, 1012, 1012, 1012, 25022, 2638, 2045, 2288, 26297, 28194, 1012, 1012, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'go',\n",
       " 'until',\n",
       " 'ju',\n",
       " '##rong',\n",
       " 'point',\n",
       " ',',\n",
       " 'crazy',\n",
       " '.',\n",
       " '.',\n",
       " 'available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bug',\n",
       " '##is',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'ci',\n",
       " '##ne',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(dataset[0].__dict__['text']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
